{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA7_Session 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roulupen-mldl/EVA7/blob/main/EVA7_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "# This class creats a Deep Learning model \n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # ?x?x1 | (3x3x1)x32 | ?x?x32 (?x? = input size)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # ?x?x32 | (3x3x1)x64 | ?x?x64 (?x? = input size)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # ?x?xn | (?/2)x(?/2)xn (n = number of channel)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # ?x?x64 | (3x3x1)x128 | ?x?x128 (?x? = input size)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) # ?x?x128 | (3x3x1)x256 | ?x?x256 (?x? = input size)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # ?x?xn | (?/2)x(?/2)xn (n = number of channel)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3) # ?x?x256 | (3x3x1)x512 | (? - 2)x(? - 2)x512 (?x? = input size)\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3) # ?x?x512 | (3x3x1)x1024 | (? - 2)x(? - 2)x1024 (?x? = input size)\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3) # ?x?x1024 | (3x3x1)x10 | (? - 2)x(? - 2)x10 (?x? = input size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # input size x = (28x28x1)\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) # 28 x 28 x 1 | 14 x 14 x 64\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))) # 14 x 14 x 64 | 7 x 7 x 256\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x)))) # 7 x 7 x 256 | 3 x 3 x 1024\n",
        "\n",
        "        x = F.relu(self.conv7(x)) # 3 x 3 x 1024 | 1 x 1 x 10\n",
        "        x = x.view(-1, 10) \n",
        "        return F.log_softmax(x) # applying softmax activation for final classification"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPXJ_eKZM_Gu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3683583-efcf-4ca0-f490-db67d9b1d7f6"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabd04f9-5426-4b5a-8579-ce88da919b60"
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) # This line normalizing the image pixels using mean and standard deviation of this image dataset(MNIST)\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) # This line normalizing the image pixels using mean and standard deviation of this image dataset(MNIST)\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ba37e4-09c4-405b-9134-d17e7d235567"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=1.99245023727417 batch_id=468: 100%|██████████| 469/469 [00:38<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9687, Accuracy: 2790/10000 (28%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUjnFdc6U9ZO"
      },
      "source": [
        "**This network is performing very badly because of the last layer, where we are directly converting 3x3x1024 into 1x1x10. Instead of convolution if we can add a fully connected layer then it'll perform better**\n",
        "\n",
        "**Reason: At the last layer the network already have parts of the images, so inorder to visualize an entire image we need to connect it together and thats why we need a fully connected layer and 2nd when we are reducing the number of channels directly from 1024 to 10 using Convolution there is loss of information.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}